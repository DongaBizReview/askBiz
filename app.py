# Streaming with Fine-Tuning model and RAG

import os, gc, sys, torch, json, subprocess

import gradio as gr

from threading      import Thread
from peft           import PeftConfig, get_peft_model, IA3Config
from transformers   import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer

from langchain.vectorstores import FAISS
from langchain.embeddings   import HuggingFaceEmbeddings
from langchain              import PromptTemplate

##### Init #####

### Loading custom settings ###
HUGGINGFACEHUB_API_TOKEN = {HUGGINGFACE_API_KEY}
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

embeddings = HuggingFaceEmbeddings(model_name = "jhgan/ko-sroberta-multitask")

### Load FAISS vector data ###
dbr_embed_chunks = FAISS.load_local({VECTOR_DATASET_DIRECTORY}, embeddings)

### Local LLM ###
peft_model_id = {MODEL_DIRECTORY}
config = PeftConfig.from_pretrained(peft_model_id)

model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path, 
    device_map = "auto", 
    load_in_8bit = True,
    torch_dtype = torch.float16,
    rope_scaling = {"type": "dynamic", "factor": 2.0}
    )
model = get_peft_model(model, config)

tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, return_token_type_ids = False)
tokenizer.pad_token = tokenizer.eos_token

##### Server #####
last_query_text = ""

def get_gpu_info(nvidia_smi_path = "nvidia-smi", no_units = True):
    
    DEFAULT_ATTRIBUTES = (
    'index',
    'uuid',
    'name',
    'timestamp',
    'memory.total',
    'memory.free',
    'memory.used',
    'utilization.gpu',
    'utilization.memory'
    )
    
    nu_opt = '' if not no_units else ',nounits'
    
    cmd = '%s --query-gpu=%s --format=csv,noheader%s' % ('nvidia-smi', ','.join(DEFAULT_ATTRIBUTES), nu_opt)
    
    output = subprocess.check_output(cmd, shell = True)
    lines = output.decode().split('\n')
    lines = [ line.strip() for line in lines if line.strip() != '' ]
    
    # gpu info
    gpu0_max_memory = int(lines[0].split(',')[4].strip())
    gpu1_max_memory = int(lines[1].split(',')[4].strip())

    gpu0_allocated_memory = int(lines[0].split(',')[6].strip())
    gpu1_allocated_memory = int(lines[1].split(',')[6].strip())

    total_alloacated_memory = gpu0_allocated_memory + gpu1_allocated_memory
    gpu_max_memory = gpu0_max_memory + gpu1_max_memory

    total_allocated_percentage = round(gpu_max_memory / total_alloacated_memory, 2)
    
    return total_allocated_percentage

def semantic_search_with_score (query, use_score=***, ref_score=***):
    use_docs = []
    ref_docs = []
    i = 0

    docs_and_scores = dbr_embed_chunks.similarity_search_with_score(query, k=10)
    
    for doc, score in docs_and_scores:
        if score <= use_score:
            use_docs.append(docs_and_scores[i])
        elif score <= ref_score:
            ref_docs.append(docs_and_scores[i])

        i += 1

    return use_docs, ref_docs
    
def print_references(user_message):
    
    if len(user_message) == 0:
        global last_query_text
        user_message = last_query_text
    
    article = dbr_embed_chunks.similarity_search_with_score(user_message, k = 5)
    
    doc_cnt = len(article)
    top1_score = article[0][-1]
    
    if top1_score >= ***:
        
        reference_log = ""
        
        return reference_log
    
    else:
        
        trg_string = "***"
        
        reference_log = [
            {
                "url" : f"***/{article[i][0].metadata['source'].split('|')[0]}" if trg_string not in article[i][0].metadata['source'] and article[i][0].metadata['source'].split(' |')[0] != "None" else "",
                "title" : article[i][0].metadata['source'].split('|')[4].strip().replace("&#8226;", "Â·") if trg_string not in article[i][0].metadata['source'] else "",
                "issue" : article[i][0].metadata['source'].split('|')[1].strip().split(' ')[0] if trg_string not in article[i][0].metadata['source'] else '|'.join(article[i][0].metadata['source'].split('|')[2:]).strip(),
                "pub_date" : article[i][0].metadata['source'].split('|')[1].strip().split(' ')[1].replace('(', '').replace(')', '') if trg_string not in article[i][0].metadata['source'] else ""
            } for i in range(doc_cnt)
        ]
        reference_log = json.dumps(reference_log, ensure_ascii = False)
                
        return reference_log


##### UI #####

def generate_by_llm_only(user_message, history):
    if not user_message:
        print('Empty Input')
        return ""
    else:
        global last_query_text
        last_query_text = user_message

    # RAG
    with torch.no_grad():
        
        gc.collect()
        torch.cuda.empty_cache()
        
        try:
            use_docs, ref_docs = semantic_search_with_score(user_message)
            similarity_score = use_docs[0][-1]
            
            if similarity_score > ***:
                prompt = "***"
                prompt = PromptTemplate.from_template(prompt)
                prompt = prompt.format(query = user_message)
                
                # print("##### prompt check : ", prompt)
                
                ## stream 
                model_inputs = tokenizer.encode(prompt, return_tensors = 'pt').to('cuda')
                streamer = TextIteratorStreamer(tokenizer, timeout = 100, skip_prompt = True, skip_special_tokens = True)

                generate_kwargs = dict(
                    inputs              = model_inputs,
                    streamer            = streamer,
                    temperature         = ***,
                    top_p               = ***,
                    top_k               = ***,
                    max_new_tokens      = ***,
                    repetition_penalty  = ***,
                    do_sample           = ***,
                    use_cache           = ***,
                    early_stopping      = ***,
                    num_beams           = ***,
                    eos_token_id        = ***,
                    pad_token_id        = ***,
                )

                t = Thread(target = model.generate, kwargs = generate_kwargs)
                t.start()

                model_output = ''
                for new_text in streamer:
                    model_output += new_text
                    yield model_output
                
                gc.collect()
                torch.cuda.empty_cache()
                
            else:
                RAG_context = use_docs[0][0].page_content
                
                # Prompt Templates
                prompt = """
                ***
                """
                prompt = PromptTemplate.from_template(prompt)
                prompt = prompt.format(query = user_message, context = RAG_context)
            
                # print("##### prompt check : ", prompt)
            
                ## stream 
                model_inputs = tokenizer.encode(prompt, return_tensors = 'pt').to('cuda')
                streamer = TextIteratorStreamer(tokenizer, timeout = 100, skip_prompt = True, skip_special_tokens = True)

                generate_kwargs = dict(
                    inputs              = model_inputs,
                    streamer            = streamer,
                    temperature         = ***,
                    top_p               = ***,
                    top_k               = ***,
                    max_new_tokens      = ***,
                    repetition_penalty  = ***,
                    do_sample           = ***,
                    use_cache           = ***,
                    early_stopping      = ***,
                    num_beams           = ***,
                    eos_token_id        = ***,
                    pad_token_id        = ***,
                )

                t = Thread(target = model.generate, kwargs = generate_kwargs)
                t.start()

                model_output = ''
                for new_text in streamer:
                    model_output += new_text
                    yield model_output
                    
                gc.collect()
                torch.cuda.empty_cache()
            
        except IndexError as ie:
            model_output = "í•´ë‹¹ ì§ˆë¬¸ì˜ ë‚´ìš©ì€ ì§€ì‹ë² ì´ìŠ¤ì— ì—†ëŠ” ì‚¬í•­ìœ¼ë¡œ ëª¨ë¸ì„ ë” ê³ ë„í™”í•˜ì—¬ ì¶”í›„ì— ë‹µë³€í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
            yield model_output
    

def clear_chat():
    return [], []

def process_example(args):
    for [x, y] in generate(args):
        pass
    return [x, y]

##### UI Variables #####

examples = [
    "ìš°ë¦¬ íšŒì‚¬ì—ì„œ ì˜¬í•´ë¶€í„° OKRì„ ë³¸ê²©ì ìœ¼ë¡œ ë„ì…í•˜ë¼ëŠ” ì§€ì‹œê°€ ë‚´ë ¤ì™”ì–´. ê·¸ëŸ°ë° ê¸°ì¡´ ì„±ê³¼ ê´€ë¦¬ ì‹œìŠ¤í…œì¸ MBO(Management by Objective)ì™€ OKRì€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê±°ì•¼?",
    "ì°©í•œ ë””ìì¸ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”?",
    "ë¡œë´‡ì„ ë„ì…í•´ ê³µì¥ ì—…ë¬´ë¥¼ ìë™í™”í•´ë³´ë ¤ê³  í•˜ëŠ”ë° ê¸°ì¡´ ì§ì›ë“¤ì´ ì ì‘ì— ì–´ë ¤ì›€ì„ ê²ªê±°ë‚˜ ë°˜ë°œí•˜ì§€ëŠ” ì•Šì„ê¹Œ?",
    "ì‹œë¦¬ì¦ˆ A ë‹¨ê³„ì˜ ì‹í’ˆ ìŠ¤íƒ€íŠ¸ì—…ì´ ì§„ì¶œí•  ë§Œí•œ í•´ì™¸ êµ­ê°€ ì¶”ì²œí•´ ì¤˜.",
    "ìŠ¤íƒ€íŠ¸ì—… ì§ì› ë³µì§€ ì‚¬ë¡€ë¥¼ ì•Œë ¤ì¤˜. ê·¸ë¦¬ê³  ì´ë“¤ ìŠ¤íƒ€íŠ¸ì—…ì€ ì™œ ë³µì§€ì— í˜ì„ ìŸëŠ” ê±¸ê¹Œ?",
]

title = """<h1 align="center">ğŸ’¬ğŸ“° AskBiz-Demo Playground ğŸ“°ğŸ’¬</h1>"""

title2 = """<div style="text-align: center; max-width: 500px; margin: 0 auto;">
                <div>
                    <h1>AskBiz Prototype</h1>
                </div>
                        <p style="margin-bottom: 10px; font-size: 94%">
                            developed by <a href="http://bigster.co.kr/">Bigster</a>
                        </p>
                </div>"""

custom_css = """
#banner-image {
    display: block;
    margin-left: auto;
    margin-right: auto;
}
#chat-message {
    font-size: 14px;
    min-height: 300px;
}
"""

if __name__ == "__main__":

    with gr.Blocks(analytics_enabled = False, css = custom_css) as demo:
        gr.HTML(title)

        with gr.Row():
            with gr.Column():
                gr.Markdown(
                    "ğŸ¤— AskBiz Streaming on gradio\n"
                    "& Current of AskBiz is a demo that showcases the model fine-tuned for LoRA"
                )

        with gr.Row():
            with gr.Box():
                output = gr.Markdown()
                # chatbot = gr.ChatBot(elem_id = 'chat-message', label = 'Chat')
                chatIf = gr.ChatInterface(generate_by_llm_only, examples=examples).queue()

        with gr.Row():
            with gr.Column(scale = 3):
                user_message = gr.Textbox(elem_id = 'q-input', show_label = False, visible = False,
                                          placeholder = "AskBizì—ê²Œ ë¬¼ì–´ë³´ê³ ì‹¶ì€ ê²ƒì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")

                with gr.Row():
                    with gr.Column(scale=1, min_width=600):
                        reference_show_btn = gr.Button("ì°¸ê³  ë‚´ìš© ìì„¸íˆ ë³´ê¸°", elem_id = 'show-reference-btn')
                        reference_log = gr.Textbox(elem_id='reference_log', label='* ëŒ€ë‹µì— ì‚¬ìš©ëœ ê¸°ì‚¬ ë° ì¶”ê°€ ì°¸ê³  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´')

                with gr.Accordion(label = "Parameters", open = False, elem_id = "h-parameters-accordion"):
                    temperature = gr.Slider(
                        label = 'Temperature',
                        value = ***,
                        minimum = 0.0,
                        maximum = 1.0,
                        step = 0.1,
                        interactive = True,
                        info = "ë†’ì€ ê°’ì„ ì„¤ì •í•˜ë©´ ë” ë‹¤ì–‘í•˜ê³  ì°½ì˜ì ì¸ ê²°ê³¼ê°’ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤"
                    )
                    top_p = gr.Slider(
                        label = 'Top-p (nuclues sampling)',
                        value = ***,
                        minimum = 0.0,
                        maximum = 1,
                        step = 0.05,
                        interactive = True,
                        info = "ë†’ì€ ê°’ì„ ì„¤ì •í•˜ë©´ ë” ë‚®ì€ ê²°ê³¼ê°’ì„ ê°€ì§€ëŠ” í† í°ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤"
                    )
                    top_k = gr.Slider(
                        label = 'Top-k sampling',
                        value = ***,
                        minimum = 0,
                        maximum = ***,
                        step = 1,
                        interactive = True,
                        info = "ìƒìœ„ kê°œì˜ ë‹¨ì–´ ì¤‘ì— ì—¬ëŸ¬ ë‹¨ì–´ê°€ ë™ì¼í•œ í™•ë¥  ê°’ì„ ê°€ì§€ë©´ ì´ë“¤ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•©ë‹ˆë‹¤"
                    )
                    max_new_tokens = gr.Slider(
                        label = 'Max new tokens',
                        value = ***,
                        minimum = ***,
                        maximum = ***,
                        step = 4,
                        interactive = True,
                        info = "ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
                    )
                    repetition_penalty = gr.Slider(
                        label = 'Repetition Penalty',
                        value = ***,
                        minimum = 0.0,
                        maximum = 10,
                        step = 0.1,
                        interactive = True,
                        info = "ë‹¨ì–´ì˜ ë“±ì¥ì„ ì œì–´í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©° ë™ì¼í•œ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šê±°ë‚˜ ìµœì†Œí™”í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"
                    )
                    
                with gr.Row():
                    gr.Markdown(
                        "Askbiz-Demoê°€ ìƒì‚°í•˜ëŠ” ê²°ê³¼ëŠ” ì •í™•í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©° ì´ì „ ë§¥ë½ì„ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤. "
                        "Askbiz-Demo modelì—” ë™ì•„ë¹„ì¦ˆë‹ˆìŠ¤ë¦¬ë·°(DBR)ì˜ ê¸°ì‚¬ ë°ì´í„°ì…‹ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ", 
                        elem_classes = ['disclaimer']
                    )
                    

        history = gr.State([])
        last_user_message = gr.State("")

        reference_show_btn.click(fn = print_references, inputs = user_message, outputs = reference_log)

    # demo.queue(concurrency_count = 16).launch(server_name = "0.0.0.0", share = True, server_port = 8081)
    demo.queue().launch(share=False, debug=False, ssl_verify=False, server_name="0.0.0.0", ssl_certfile="cert.pem", ssl_keyfile="key.pem", server_port = 7860)